\section{Uncertainty Analysis, Counting Statistics, and Detection}
\subsection{Uncertainty Analysis}
\subsubsection{Characterization of Data}
With $N$ independent measurements of the same physical quantity:
\begin{itemize}
    \item Experimental mean: $\overline{x_e}=\frac{1}{N}\sum_{i=1}^Nx_i$
    \item Residual: $d_i=x_i-\overline{x_e},\text{ with }\sum_{i=1}^Nd_i=0$
    \item Deviation: $\epsilon_i=x_i-\overline{x}$, note that $\overline{x}$ is the true mean 
    \item Sample variance: $s^2=\overline{\epsilon^2}=\frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^2=\frac{1}{N-1}\sum_{i=1}^N(x_i-\overline{x_e})^2$
    \item Standard Deviation: $\sigma^2=\frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^2=\frac{1}{N-1}\sum_{i=1}^N(x_i-\overline{x_e})^2$, or $\sigma^2\approx\overline{x_i^2}-(\overline{x_e})^2$\\
    The standard deviation is also known as the external uncertainty in the mean.
\end{itemize}
\subsubsection{Internal and External Uncertainties}
\begin{itemize}
    \item The uncertainty in the mean (internal uncertainty) is $\sigma_{\text{internal}}=\sigma/\sqrt{N}$
    \item Assuming different uncertainties $\sigma_i$ for $x_i$:
    \begin{itemize}
        \item The weighted mean $\overline{x_w}=\left(\sum_{i=1}^Nx_i\right)/\left(\sigma_i^2\sum_{i=1}^N1/\sigma_i^2\right)$
        \item The error in the weighted mean $\sigma_{\text{internal}}=1/\sqrt{\sum_{i=1}^N1/\sigma_i^2}$\\, or $\left(1/\sigma_\text{internal}^2\right)=\sum_{i=1}^N\left(1/\sigma_{x_i}^2\right)$
        \item The external uncertainty $\sigma_\text{external}=\sigma_\text{internal}\sqrt{\frac{\sum(x_i-\overline{x})^2/\sigma_i^2}{N-1}}$
    \end{itemize}
    
\end{itemize}
\subsubsection{Error Propagation Rules}
\begin{itemize}
    \item Assume $N$ independent measurements of the same physical quantity, to which $F$ is related by $F=F(x_1,x_2,...)$:\\
    then the uncertainty in F is given by $\sigma_F^2=\sum_{i=1}^N\left(\frac{\partial F}{\partial x_i}\right)^2\sigma^2_{x_i}$
    \item Examples
    \begin{itemize}
        \item Sum and differences: $\sigma_{x+y}=\sigma_{x-y}=\sqrt{\sigma_x^2+\sigma_y^2}$
        \item Multiplication and division by constant: $\sigma_{Kx}=K\sigma_x$
        \item Multiplication: $\sigma_{x\cdot y}=(x\cdot y)\sqrt{\left(\sigma_x/x\right)^2+\left(\sigma_y/y\right)^2}$
        \item Division: $\sigma_{x/y}=({x}/{y})\sqrt{\left(\sigma_x/x\right)^2+\left(\sigma_y/y\right)^2}$
    \end{itemize}
    \item Counting Rate ($R$)
    \begin{itemize}
        \item[] $R=n/T$ and $\sigma_n=\sqrt{n}\;\Rightarrow\;\sigma_R=\sqrt{n}/{T}=\sqrt{R/T}$
        \item[] $R_S=R_{S+B}-R_B\;\Rightarrow\;\sigma_{R_S}=\sqrt{\frac{n_{S+B}}{T_{S+B}^2}+\frac{n_{B}}{T_{B}^2}}=\sqrt{\frac{R_{S+B}}{T_{S+B}}+\frac{R_B}{T_B}}$
    \end{itemize}
\end{itemize}
\subsubsection{Rounding of Values}
Rounding to 1-2 significant figures in the uncertainty, demonstrated by the following examples:
\begin{center}
\begin{tabular}{c c c}
    Raw value & $n=1$ & $n=2$\\
    \hline
    $5.73297251\pm0.01477602$ &  $5.73\pm0.01$ & $5.733\pm0.015$ \\
    $314775089\pm4500284$ & $(3.15\pm0.05)\times10^8$ & $(3.148\pm0.045)\times10^8$ \\
    $255\pm73$ & $(2.6\pm0.7)\times10^2$ & $255\pm73$
\end{tabular}  
\end{center}
\subsection{Counting Statistics}
\subsubsection{Statistical Models}
\begin{itemize}
    \item Binomial distribution
    \begin{itemize}
        \item The most general of all listed here. Given probability $p$ of an event in a single trial, the probability of $x$ events in $n$ trials is given by\\
        $P(x)=\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$
        \item[] $\overline{x}=np$
        \item[] $\sigma^2=np(1-p)=\overline{x}(1-p)$
        \item[] $\sum_{x=0}^nP(x)=1$
        \item Though computationally intensive, radioactive decay can be described with a very large $n$ and $p=1-\exp(-\lambda t)$
    \end{itemize}
    \item Poisson distribution
    \begin{itemize}
        \item If $p$ of the binomial distribution is small and constant ($p\ll1$), it can be reduced to the Poisson distribution. For example, when a count interval is short compared with the half-life of the source.
        \item[] $P(x)=\frac{(\overline{x})^xe^{-\overline{x}}}{x!}$
        \item[] $\overline{x}=np$
        \item[] $\sigma^2=np=\overline{x}$
        \item[] $\sum_{x=0}^nP(x)=1$
        \item Note that to characterize a Poisson distribution, only one parameter is needed, that is the product of $n$ and $p$. With only the mean value of the distribution ($\overline{x}$), the entire distribution can be constructed.
    \end{itemize}
    \item Normal distribution
    \begin{itemize}
        \item If in addition to $p\ll1$, the mean value of the distribution is large enough ($np\gg1$), the distribution can be further simplified to a normal (Gaussian) distribution.
        \item[] $P(x)=\frac{1}{\sqrt{2\pi\overline{x}}}\exp\left(-\frac{(x-\overline{x})^2}{2\overline{x}}\right)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\overline{x})^2}{2\sigma^2}\right)$
        \item[] $\overline{x}=np$
        \item[] $\sigma^2=np=\overline{x}$
        \item[] $\sum P(x)=1$
    \end{itemize}
\end{itemize}
\subsubsection{Confidence Limits}
\begin{itemize}
    \item Estimation of the precision of a single measurement
    \item Confidence limits
\end{itemize}
\subsubsection{The \texorpdfstring{$\chi^2$}{Chi-squared} Test}
\subsubsection{Examples}
\begin{enumerate}
    \item A series of measurements
    \item Data with different errors
    \item Count rates
    \item Consecutive counting
    \item The null Experiment
\end{enumerate}
\subsection{Detection}
\subsubsection{Dead Time}
\subsubsection{Decision Making}